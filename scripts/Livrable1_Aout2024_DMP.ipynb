{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "\n",
    "import scipy.stats as scs\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 0: Chargement de donnees Aging_INDICES & Tractometry_allSubs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Aging_INDICES = pd.read_csv('Aging_INDICES.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracto_folder_path = './Tractometry_allSubs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1: Filtrage des groupes \"jeune\" et \"vieux\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtrer les sujets du groupe \"Y\" (jeunes)\n",
    "data_young_sub = Aging_INDICES[Aging_INDICES['group'] == 'Y']\n",
    "list_young_sub = data_young_sub[\"sub\"].tolist()\n",
    "\n",
    "# Filtrer les sujets du groupe \"O\" (vieux)\n",
    "data_old_sub = Aging_INDICES[Aging_INDICES['group'] == 'O']\n",
    "list_old_sub = data_old_sub[\"sub\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2: Fonction de segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segmentation(dti_metric, list_sub):\n",
    "    list_CC4_val, list_CC5_val = [], []\n",
    "    \n",
    "    for sub in list_sub:\n",
    "        file_path = os.path.join(tracto_folder_path, f'{sub}_{dti_metric}_tractometry.csv')\n",
    "        \n",
    "        try:\n",
    "            data_sub = pd.read_csv(file_path, sep=\";\")\n",
    "            list_CC4_val.append(data_sub['CC_4'].values)\n",
    "            list_CC5_val.append(data_sub['CC_5'].values)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Fichier {file_path} introuvable.\")\n",
    "        except KeyError:\n",
    "            print(f\"Colonnes 'CC_4' ou 'CC_5' manquantes dans {file_path}.\")\n",
    "    \n",
    "    return list_CC4_val, list_CC5_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3: Extraction pour chaque variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extraction pour la variable FA\n",
    "list_CC4_FA_young_val, list_CC5_FA_young_val = segmentation('FA', list_young_sub)\n",
    "list_CC4_FA_old_val, list_CC5_FA_old_val = segmentation('FA', list_old_sub)\n",
    "\n",
    "# Extraction pour la variable MD\n",
    "list_CC4_MD_young_val, list_CC5_MD_young_val = segmentation('MD', list_young_sub)\n",
    "list_CC4_MD_old_val, list_CC5_MD_old_val = segmentation('MD', list_old_sub)\n",
    "\n",
    "# Extraction pour la variable RD\n",
    "list_CC4_RD_young_val, list_CC5_RD_young_val = segmentation('RD', list_young_sub)\n",
    "list_CC4_RD_old_val, list_CC5_RD_old_val = segmentation('RD', list_old_sub)\n",
    "\n",
    "# Extraction pour la variable AD\n",
    "list_CC4_AD_young_val, list_CC5_AD_young_val = segmentation('AD', list_young_sub)\n",
    "list_CC4_AD_old_val, list_CC5_AD_old_val = segmentation('AD', list_old_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4: Fonction de binning des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bin_data(data, num_bins):\n",
    "    binned_data = []\n",
    "    for subject_data in data:\n",
    "        subject_data = np.array(subject_data, dtype=np.float64)\n",
    "        splits = np.array_split(subject_data, num_bins)\n",
    "        bin_means = [np.mean(spl) for spl in splits]\n",
    "        binned_data.append(bin_means)\n",
    "    return np.array(binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_bins = 12\n",
    "\n",
    "# Binning des données FA pour CC4\n",
    "list_CC4_FA_young_binned = bin_data(list_CC4_FA_young_val, num_bins)\n",
    "list_CC4_FA_old_binned = bin_data(list_CC4_FA_old_val, num_bins)\n",
    "\n",
    "# Binning des données FA pour CC5\n",
    "list_CC5_FA_young_binned = bin_data(list_CC5_FA_young_val, num_bins)\n",
    "list_CC5_FA_old_binned = bin_data(list_CC5_FA_old_val, num_bins)\n",
    "\n",
    "# Binning des données MD pour CC4\n",
    "list_CC4_MD_young_binned = bin_data(list_CC4_MD_young_val, num_bins)\n",
    "list_CC4_MD_old_binned = bin_data(list_CC4_MD_old_val, num_bins)\n",
    "\n",
    "# Binning des données MD pour CC5\n",
    "list_CC5_MD_young_binned = bin_data(list_CC5_MD_young_val, num_bins)\n",
    "list_CC5_MD_old_binned = bin_data(list_CC5_MD_old_val, num_bins)\n",
    "\n",
    "# Binning des données RD pour CC4\n",
    "list_CC4_RD_young_binned = bin_data(list_CC4_RD_young_val, num_bins)\n",
    "list_CC4_RD_old_binned = bin_data(list_CC4_RD_old_val, num_bins)\n",
    "\n",
    "# Binning des données RD pour CC5\n",
    "list_CC5_RD_young_binned = bin_data(list_CC5_RD_young_val, num_bins)\n",
    "list_CC5_RD_old_binned = bin_data(list_CC5_RD_old_val, num_bins)\n",
    "\n",
    "# Binning des données AD pour CC4\n",
    "list_CC4_AD_young_binned = bin_data(list_CC4_AD_young_val, num_bins)\n",
    "list_CC4_AD_old_binned = bin_data(list_CC4_AD_old_val, num_bins)\n",
    "\n",
    "# Binning des données AD pour CC5\n",
    "list_CC5_AD_young_binned = bin_data(list_CC5_AD_young_val, num_bins)\n",
    "list_CC5_AD_old_binned = bin_data(list_CC5_AD_old_val, num_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_combined_dataframe(list_young_binned, list_old_binned, variable_name, cc_name):\n",
    "    \"\"\"\n",
    "    Fonction pour combiner les données binned des jeunes et des vieux et créer un DataFrame.\n",
    "    \"\"\"\n",
    "    combined_data = np.vstack([list_young_binned, list_old_binned])\n",
    "    \n",
    "    df_combined = pd.DataFrame(combined_data, \n",
    "                               columns=[f'Bin_{i+1}' for i in range(combined_data.shape[1])])\n",
    "    \n",
    "    df_combined['Group'] = ['Young'] * len(list_young_binned) + ['Old'] * len(list_old_binned)\n",
    "    \n",
    "    df_combined['Variable'] = variable_name\n",
    "    df_combined['CC_Segment'] = cc_name\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# FA - CC4\n",
    "df_CC4_FA_combined = create_combined_dataframe(list_CC4_FA_young_binned, list_CC4_FA_old_binned, 'FA', 'CC4')\n",
    "\n",
    "# FA - CC5\n",
    "df_CC5_FA_combined = create_combined_dataframe(list_CC5_FA_young_binned, list_CC5_FA_old_binned, 'FA', 'CC5')\n",
    "\n",
    "# MD - CC4\n",
    "df_CC4_MD_combined = create_combined_dataframe(list_CC4_MD_young_binned, list_CC4_MD_old_binned, 'MD', 'CC4')\n",
    "\n",
    "# MD - CC5\n",
    "df_CC5_MD_combined = create_combined_dataframe(list_CC5_MD_young_binned, list_CC5_MD_old_binned, 'MD', 'CC5')\n",
    "\n",
    "# RD - CC4\n",
    "df_CC4_RD_combined = create_combined_dataframe(list_CC4_RD_young_binned, list_CC4_RD_old_binned, 'RD', 'CC4')\n",
    "\n",
    "# RD - CC5\n",
    "df_CC5_RD_combined = create_combined_dataframe(list_CC5_RD_young_binned, list_CC5_RD_old_binned, 'RD', 'CC5')\n",
    "\n",
    "# AD - CC4\n",
    "df_CC4_AD_combined = create_combined_dataframe(list_CC4_AD_young_binned, list_CC4_AD_old_binned, 'AD', 'CC4')\n",
    "\n",
    "# AD - CC5\n",
    "df_CC5_AD_combined = create_combined_dataframe(list_CC5_AD_young_binned, list_CC5_AD_old_binned, 'AD', 'CC5')\n",
    "\n",
    "df_all_combined = pd.concat([df_CC4_FA_combined, df_CC5_FA_combined, \n",
    "                             df_CC4_MD_combined, df_CC5_MD_combined, \n",
    "                             df_CC4_RD_combined, df_CC5_RD_combined, \n",
    "                             df_CC4_AD_combined, df_CC5_AD_combined], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5: Création du jeu de données pour un segment et une variable\n",
    "\n",
    "##### Cette étape consiste à filtrer les données en fonction d'un **segment** (par exemple, **CC4** ou **CC5**) et d'une **variable** (par exemple, **FA**, **MD**, **RD**, ou **AD**). Le but est de constituer un sous-ensemble de données spécifiques à cette combinaison afin de préparer les données pour une analyse ultérieure, telle que le clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtered = df_all_combined[(df_all_combined['Variable'] == 'FA') & (df_all_combined['CC_Segment'] == 'CC4')]\n",
    "df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1)\n",
    "df_filtered_clean = df_filtered_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "#df_used = pd.concat([Aging_INDICES], axis=1, ignore_index=False)\n",
    "df_used = df_used.drop(['sub', 'sex','group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', 'FAsigCC5','ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', 'RH_GMvM1', 'RH_GMvS1', 'GMt_M1','GMt_S1', 'LH_GM_M1S1', 'RH_GM_M1S1'], axis=1)\n",
    "df_used = df_used.drop(['TGMv_all','ctxGMv_all','LH_GMv_all', 'RH_GMv_all'], axis=1)\n",
    "df_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann-Whitney U Test with FDR Correction on Imputed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "\n",
    "results = []\n",
    "p_values = []\n",
    "\n",
    "for col in df_used.columns:\n",
    "    col1 = df_used[col][Aging_INDICES[\"group\"] == \"Y\"]  # Groupe Y (jeunes)\n",
    "    col2 = df_used[col][Aging_INDICES[\"group\"] == \"O\"]  # Groupe O (vieux)\n",
    "    \n",
    "    # Test de Mann-Whitney U\n",
    "    test_result = scs.mannwhitneyu(col1, col2)\n",
    "    \n",
    "    results.append(test_result[0].round(4))  # U-statistique\n",
    "    p_values.append(test_result[1])          # p-value\n",
    "\n",
    "# Correction pour tests multiples (alpha 5% et 1%)\n",
    "c1 = multipletests(p_values, alpha=0.05, method='fdr_bh')  # Correction avec FDR (Benjamini-Hochberg)\n",
    "c2 = multipletests(p_values, alpha=0.01, method='fdr_bh')  # Correction avec FDR (Benjamini-Hochberg)\n",
    "\n",
    "res = pd.DataFrame({\n",
    "    \"col_name\": df_used.columns,      # Noms des colonnes\n",
    "    \"U-stat\": results,                # U-statistiques\n",
    "    \"pvalue\": p_values,               # p-values\n",
    "    \"adjusted_p_value\": c1[1],        # p-values ajustées (alpha 5%)\n",
    "    \"sign_5%\": c1[0],                 # Significatif à 5%\n",
    "    \"sign_1%\": c2[0]                  # Significatif à 1%\n",
    "})\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "### Daniela\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "### Youssef\n",
    "\n",
    "**I still need to complete the study for 12 bins.**\n",
    "\n",
    "**I still need to complete the study on the variables -> I will replace the bins with statistical variables instead**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Préparation des données**\n",
    "\n",
    "### a. Importation des librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Chargement et visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_used.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualisation de la distribution des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_used.hist(figsize=(15, 10), bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Analyse de clustering**\n",
    "\n",
    "### a. Standardisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_used)\n",
    "df_scaled_df = pd.DataFrame(df_scaled, columns=df_used.columns)\n",
    "df_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_used.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Clustering avec **K-means**\n",
    "\n",
    "1. **Choix du nombre de clusters (K)** avec l'inertie et le score de silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choix du nombre de clusters K\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)  \n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertia.append(kmeans.inertia_)  # Inertie (cohésion)\n",
    "    silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))  # Score de silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualisation du nombre de clusters optimal (Méthode du coude)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(K_range, inertia, marker='o')\n",
    "plt.title(\"Méthode du coude : Inertie vs Nombre de Clusters (K)\")\n",
    "plt.xlabel(\"Nombre de Clusters (K)\")\n",
    "plt.ylabel(\"Inertie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualisation du score de silhouette\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(K_range, silhouette_scores, marker='o')\n",
    "plt.title(\"Score de Silhouette vs Nombre de Clusters (K)\")\n",
    "plt.xlabel(\"Nombre de Clusters (K)\")\n",
    "plt.ylabel(\"Score de Silhouette\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Exécution du clustering avec K-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Appliquer K-means avec K=3\n",
    "kmeans = KMeans(n_clusters=3 , random_state=42)\n",
    "kmeans.fit(df_scaled)\n",
    "df_used['Cluster'] = kmeans.labels_\n",
    "df_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Interprétation des clusters**\n",
    "\n",
    "### a. Analyse des variables par cluster (K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Statistiques descriptives par cluster (K-means)\n",
    "cluster_stats = df_used.groupby('Cluster').mean()\n",
    "cluster_stats.T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Visualisation des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame({\n",
    "    'actual': Aging_INDICES[\"group\"], \n",
    "    'predicted': df_used['Cluster']    \n",
    "})\n",
    "df_clusters = df_clusters.sort_values(by='predicted')\n",
    "group_distribution = pd.crosstab(df_clusters['predicted'], df_clusters['actual'])\n",
    "group_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_distribution.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "plt.title(\"Répartition des groupes Young et Old dans chaque Cluster (K-means)\")\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Nombre d'individus\")\n",
    "plt.legend(title=\"Groupes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code complet pour appliquer sur toutes les combinaisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables et segments à traiter\n",
    "variables = ['FA', 'MD', 'RD', 'AD']\n",
    "segments = ['CC4', 'CC5']\n",
    "cluster_results = {}\n",
    "\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "for variable in variables:\n",
    "    for segment in segments:\n",
    "\n",
    "        df_filtered = df_all_combined[(df_all_combined['Variable'] == variable) & (df_all_combined['CC_Segment'] == segment)]\n",
    "        df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1)\n",
    "        df_filtered_clean = df_filtered_clean.reset_index(drop=True)\n",
    "        df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "        df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "        columns_to_drop = ['sub', 'sex', 'group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', \n",
    "                           'FAsigCC5', 'ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', \n",
    "                           'RH_GMvM1', 'RH_GMvS1', 'GMt_M1','GMt_S1', 'LH_GM_M1S1', 'RH_GM_M1S1']\n",
    "        df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(df_used)\n",
    "        df_scaled_df = pd.DataFrame(df_scaled, columns=df_used.columns)\n",
    "        \n",
    "        inertia = []\n",
    "        silhouette_scores = []\n",
    "        K_range = range(2, 11)\n",
    "        \n",
    "        for k in K_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(df_scaled)\n",
    "            inertia.append(kmeans.inertia_)  # Inertie\n",
    "            silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))  # Score de silhouette\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        kmeans.fit(df_scaled)\n",
    "        \n",
    "        df_used['Cluster'] = kmeans.labels_\n",
    "        df_clusters = pd.DataFrame({\n",
    "            'actual': Aging_INDICES[\"group\"], \n",
    "            'predicted': df_used['Cluster']    \n",
    "        })\n",
    "        df_clusters = df_clusters.sort_values(by='predicted')\n",
    "        group_distribution = pd.crosstab(df_clusters['predicted'], df_clusters['actual'])\n",
    "        cluster_results[f'{variable}_{segment}'] = group_distribution\n",
    "        \n",
    "        group_distribution.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "        plt.title(f\"Répartition des groupes Young et Old dans chaque Cluster (K-means) pour {variable} et {segment}\")\n",
    "        plt.xlabel(\"Clusters\")\n",
    "        plt.ylabel(\"Nombre d'individus\")\n",
    "        plt.legend(title=\"Groupes\")\n",
    "        plt.show()\n",
    "\n",
    "for key, result in cluster_results.items():\n",
    "    print(f\"\\nRépartition pour {key}:\\n\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering : Agglomerative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['FA', 'MD', 'RD', 'AD']\n",
    "segments = ['CC4', 'CC5']\n",
    "cluster_results = {}\n",
    "\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "for variable in variables:\n",
    "    for segment in segments:\n",
    "\n",
    "        df_filtered = df_all_combined[(df_all_combined['Variable'] == variable) & (df_all_combined['CC_Segment'] == segment)]\n",
    "        \n",
    "        df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1)\n",
    "        df_filtered_clean = df_filtered_clean.reset_index(drop=True)\n",
    "        \n",
    "        df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "        \n",
    "        df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "        \n",
    "        columns_to_drop = ['sub', 'sex', 'group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', \n",
    "                           'FAsigCC5', 'ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', \n",
    "                           'RH_GMvM1', 'RH_GMvS1', 'GMt_M1','GMt_S1', 'LH_GM_M1S1', 'RH_GM_M1S1']\n",
    "        df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(df_used)\n",
    "        df_scaled_df = pd.DataFrame(df_scaled, columns=df_used.columns)\n",
    "        \n",
    "        silhouette_scores = []\n",
    "        K_range = range(2, 11)\n",
    "        \n",
    "        for k in K_range:\n",
    "            agg_clustering = AgglomerativeClustering(n_clusters=k)\n",
    "            labels = agg_clustering.fit_predict(df_scaled)\n",
    "            \n",
    "            if len(set(labels)) > 1:  \n",
    "                silhouette_avg = silhouette_score(df_scaled, labels)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "            else:\n",
    "                silhouette_scores.append(-1) \n",
    "        \n",
    "        best_k = K_range[np.argmax(silhouette_scores)]\n",
    "        print(f\"Meilleur nombre de clusters pour {variable} et {segment}: {best_k}\")\n",
    "        \n",
    "        agg_clustering = AgglomerativeClustering(n_clusters=4)\n",
    "        labels = agg_clustering.fit_predict(df_scaled)\n",
    "        df_used['Cluster'] = labels\n",
    "        \n",
    "        df_clusters = pd.DataFrame({\n",
    "            'actual': Aging_INDICES[\"group\"], \n",
    "            'predicted': df_used['Cluster']    \n",
    "        })\n",
    "        df_clusters = df_clusters.sort_values(by='predicted')\n",
    "        \n",
    "        group_distribution = pd.crosstab(df_clusters['predicted'], df_clusters['actual'])\n",
    "        cluster_results[f'{variable}_{segment}'] = group_distribution\n",
    "        \n",
    "        group_distribution.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "        plt.title(f\"Répartition des groupes Young et Old dans chaque Cluster (Agglomerative) pour {variable} et {segment}\")\n",
    "        plt.xlabel(\"Clusters\")\n",
    "        plt.ylabel(\"Nombre d'individus\")\n",
    "        plt.legend(title=\"Groupes\")\n",
    "        plt.show()\n",
    "\n",
    "for key, result in cluster_results.items():\n",
    "    print(f\"\\nRépartition pour {key}:\\n\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianMixture Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Variables et segments à traiter\n",
    "variables = ['FA', 'MD', 'RD', 'AD']\n",
    "segments = ['CC4', 'CC5']\n",
    "n_clusters = 3\n",
    "cluster_results = {}\n",
    "\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "for variable in variables:\n",
    "    for segment in segments:\n",
    "\n",
    "        df_filtered = df_all_combined[(df_all_combined['Variable'] == variable) & (df_all_combined['CC_Segment'] == segment)]\n",
    "        df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1)\n",
    "        df_filtered_clean = df_filtered_clean.reset_index(drop=True)\n",
    "        df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "        df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "        columns_to_drop = ['sub', 'sex', 'group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', \n",
    "                           'FAsigCC5', 'ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', \n",
    "                           'RH_GMvM1', 'RH_GMvS1', 'GMt_M1','GMt_S1', 'LH_GM_M1S1', 'RH_GM_M1S1']\n",
    "        df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(df_used)\n",
    "        df_scaled_df = pd.DataFrame(df_scaled, columns=df_used.columns)\n",
    "        \n",
    "        inertia = []\n",
    "        silhouette_scores = []\n",
    "        K_range = range(2, 11)\n",
    "        \n",
    "        for k in K_range:\n",
    "            gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "            gmm.fit(df_scaled)\n",
    "            labels = gmm.predict(df_scaled)\n",
    "            silhouette_scores.append(silhouette_score(df_scaled, labels))  # Score de silhouette\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "        gmm.fit(df_scaled)\n",
    "        df_used['Cluster'] = gmm.predict(df_scaled)\n",
    "        \n",
    "        df_clusters = pd.DataFrame({\n",
    "            'actual': Aging_INDICES[\"group\"], \n",
    "            'predicted': df_used['Cluster']    \n",
    "        })\n",
    "        df_clusters = df_clusters.sort_values(by='predicted')\n",
    "        group_distribution = pd.crosstab(df_clusters['predicted'], df_clusters['actual'])\n",
    "        cluster_results[f'{variable}_{segment}'] = group_distribution\n",
    "        \n",
    "        group_distribution.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "        plt.title(f\"Répartition des groupes Young et Old dans chaque Cluster (Gaussian Mixture) pour {variable} et {segment}\")\n",
    "        plt.xlabel(\"Clusters\")\n",
    "        plt.ylabel(\"Nombre d'individus\")\n",
    "        plt.legend(title=\"Groupes\")\n",
    "        plt.show()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for key, result in cluster_results.items():\n",
    "    print(f\"\\nRépartition pour {key}:\\n\")\n",
    "    print(result)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, SpectralClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "variables = ['FA', 'MD', 'RD', 'AD']\n",
    "segments = ['CC4', 'CC5']\n",
    "cluster_results = {}\n",
    "\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "def dbscan_clustering(df_scaled, epsilon=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Applique l'algorithme DBSCAN pour le clustering.\n",
    "    \"\"\"\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(df_scaled)\n",
    "    return labels\n",
    "\n",
    "def spectral_clustering(df_scaled, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Applique l'algorithme Spectral Clustering pour le clustering.\n",
    "    \"\"\"\n",
    "    spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', random_state=42)\n",
    "    labels = spectral.fit_predict(df_scaled)\n",
    "    return labels\n",
    "\n",
    "def plot_cluster_distribution(df_clusters, variable, segment, method):\n",
    "    \"\"\"\n",
    "    Visualise la répartition des groupes dans les clusters.\n",
    "    \"\"\"\n",
    "    group_distribution = pd.crosstab(df_clusters['predicted'], df_clusters['actual'])\n",
    "    group_distribution.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "    plt.title(f\"Répartition des groupes Young et Old dans chaque Cluster ({method}) pour {variable} et {segment}\")\n",
    "    plt.xlabel(\"Clusters\")\n",
    "    plt.ylabel(\"Nombre d'individus\")\n",
    "    plt.legend(title=\"Groupes\")\n",
    "    plt.show()\n",
    "    return group_distribution\n",
    "\n",
    "for variable in variables:\n",
    "    for segment in segments:\n",
    "\n",
    "        df_filtered = df_all_combined[(df_all_combined['Variable'] == variable) & (df_all_combined['CC_Segment'] == segment)]\n",
    "        df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1)\n",
    "        df_filtered_clean = df_filtered_clean.reset_index(drop=True)\n",
    "        df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "        df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "        columns_to_drop = ['sub', 'sex', 'group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', \n",
    "                           'FAsigCC5', 'ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', \n",
    "                           'RH_GMvM1', 'RH_GMvS1', 'GMt_M1', 'GMt_S1', 'LH_GM_M1S1', 'RH_GM_M1S1']\n",
    "        df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(df_used)\n",
    "\n",
    "        neighbors = NearestNeighbors(n_neighbors=5)\n",
    "        neighbors_fit = neighbors.fit(df_scaled)\n",
    "        distances, indices = neighbors_fit.kneighbors(df_scaled)\n",
    "        distances = np.sort(distances[:, 4])  # 4 correspond à n_neighbors - 1\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(distances)\n",
    "        plt.title(f\"Courbe pour déterminer epsilon pour {variable} et {segment}\")\n",
    "        plt.xlabel(\"Points triés par distance\")\n",
    "        plt.ylabel(\"Distance au 5e plus proche voisin\")\n",
    "        plt.show()\n",
    "\n",
    "        epsilon = 0.5  # À ajuster selon la courbe des distances\n",
    "        min_samples = 5\n",
    "        labels_dbscan = dbscan_clustering(df_scaled, epsilon, min_samples)\n",
    "        \n",
    "        # Spectral Clustering\n",
    "        n_clusters = 4  # À ajuster selon les besoins\n",
    "        labels_spectral = spectral_clustering(df_scaled, n_clusters)\n",
    "\n",
    "        df_used['DBSCAN_Cluster'] = labels_dbscan\n",
    "        df_used['Spectral_Cluster'] = labels_spectral\n",
    "\n",
    "        valid_clusters = df_used[df_used['DBSCAN_Cluster'] != -1]\n",
    "        valid_spectral = df_used[df_used['Spectral_Cluster'] != -1]\n",
    "        \n",
    "        if len(set(valid_clusters['DBSCAN_Cluster'])) > 1:\n",
    "            df_clusters_dbscan = pd.DataFrame({\n",
    "                'actual': Aging_INDICES.loc[valid_clusters.index, \"group\"],\n",
    "                'predicted': valid_clusters['DBSCAN_Cluster']    \n",
    "            })\n",
    "            group_distribution_dbscan = plot_cluster_distribution(df_clusters_dbscan, variable, segment, method='DBSCAN')\n",
    "\n",
    "        if len(set(valid_spectral['Spectral_Cluster'])) > 1:\n",
    "            df_clusters_spectral = pd.DataFrame({\n",
    "                'actual': Aging_INDICES.loc[valid_spectral.index, \"group\"],\n",
    "                'predicted': valid_spectral['Spectral_Cluster']    \n",
    "            })\n",
    "            group_distribution_spectral = plot_cluster_distribution(df_clusters_spectral, variable, segment, method='Spectral Clustering')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for key, result in cluster_results.items():\n",
    "    print(f\"\\nRépartition pour {key}:\\n\")\n",
    "    print(result)\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "### Youssef\n",
    "\n",
    "**I still need to conduct a study without the bins, using clusters, to see the impact of the dataframe bins on the 8 selected variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables et segments à traiter\n",
    "variables = ['FA']\n",
    "segments = ['CC5']\n",
    "n_clusters = 3\n",
    "cluster_results = {}\n",
    "\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "for variable in variables:\n",
    "    for segment in segments:\n",
    "\n",
    "        df_filtered = df_all_combined[(df_all_combined['Variable'] == variable) & (df_all_combined['CC_Segment'] == segment)]\n",
    "        df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1)\n",
    "        df_filtered_clean = df_filtered_clean.reset_index(drop=True)\n",
    "        df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "        df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "        columns_to_drop = ['sub', 'sex', 'group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', \n",
    "                           'FAsigCC5', 'ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', \n",
    "                           'RH_GMvM1', 'RH_GMvS1', 'GMt_M1','GMt_S1', 'LH_GM_M1S1', 'RH_GM_M1S1']\n",
    "        df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(df_used)\n",
    "        df_scaled_df = pd.DataFrame(df_scaled, columns=df_used.columns)\n",
    "        \n",
    "        inertia = []\n",
    "        silhouette_scores = []\n",
    "        K_range = range(2, 11)\n",
    "        \n",
    "        for k in K_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(df_scaled)\n",
    "            inertia.append(kmeans.inertia_)  # Inertie\n",
    "            silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))  # Score de silhouette\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        kmeans.fit(df_scaled)\n",
    "        \n",
    "        df_used['Cluster'] = kmeans.labels_\n",
    "        df_clusters = pd.DataFrame({\n",
    "            'actual': Aging_INDICES[\"group\"], \n",
    "            'predicted': df_used['Cluster']    \n",
    "        })\n",
    "        df_clusters = df_clusters.sort_values(by='predicted')\n",
    "        group_distribution = pd.crosstab(df_clusters['predicted'], df_clusters['actual'])\n",
    "        cluster_results[f'{variable}_{segment}'] = group_distribution\n",
    "        \n",
    "        group_distribution.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "        plt.title(f\"Répartition des groupes Young et Old dans chaque Cluster (K-means) pour {variable} et {segment}\")\n",
    "        plt.xlabel(\"Clusters\")\n",
    "        plt.ylabel(\"Nombre d'individus\")\n",
    "        plt.legend(title=\"Groupes\")\n",
    "        plt.show()\n",
    "        \n",
    "        df_used = pd.concat([Aging_INDICES[['sub', 'sex', 'group']], df_used], axis=1, ignore_index=False)\n",
    "        for cluster_num in range(n_clusters):  # Boucle sur chaque cluster\n",
    "            print(f\"\\nDataFrame pour Cluster {cluster_num} ({variable}, {segment}):\")\n",
    "            cluster_data = df_used[df_used['Cluster'] == cluster_num]\n",
    "            cluster_data = cluster_data.drop(columns=['sex', 'group'], errors='ignore')\n",
    "            cluster_data = cluster_data.merge(Aging_INDICES[['sub', 'sex', 'group']], on='sub', how='left')\n",
    "            cluster_data = cluster_data[['sub', 'sex', 'group'] + [col for col in cluster_data.columns if col not in ['sub', 'sex', 'group']]]\n",
    "            display(cluster_data)\n",
    "\n",
    "\"\"\"\n",
    "# Affichage des résultats de la répartition des clusters\n",
    "for key, result in cluster_results.items():\n",
    "    print(f\"\\nRépartition pour {key}:\\n\")\n",
    "    print(result)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Question: Quelles caractéristiques contribuent le plus à la séparation entre les clusters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "def SHAP_features_importance(stat_df, kmeans, scaler):\n",
    "    def model_output(data):\n",
    "        return kmeans.transform(data)\n",
    "\n",
    "    stat_df_scaled = scaler.transform(stat_df)\n",
    "    explainer = shap.KernelExplainer(model_output, stat_df_scaled)\n",
    "    shap_values = explainer.shap_values(stat_df_scaled)\n",
    "    feature_importance = np.mean(np.abs(shap_values), axis=0).mean(axis=1)\n",
    "    if len(feature_importance) != stat_df.shape[1]:\n",
    "        raise ValueError(f\"La longueur des SHAP values ({len(feature_importance)}) ne correspond pas au nombre de colonnes du DataFrame ({stat_df.shape[1]}).\")\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': stat_df.columns,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "    plt.title('SHAP Feature Importance for KMeans Clustering')\n",
    "    plt.show()\n",
    "\n",
    "    shap.summary_plot(shap_values, features=stat_df_scaled, feature_names=stat_df.columns)\n",
    "\n",
    "def run_shap_analysis(df_combined, Aging_INDICES, variable, segment, n_clusters=3):\n",
    "    df_filtered = df_combined[(df_combined['Variable'] == variable) & (df_combined['CC_Segment'] == segment)]\n",
    "    df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1).reset_index(drop=True)\n",
    "    df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "    df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "    columns_to_drop = ['sub', 'sex', 'group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', \n",
    "                       'FAsigCC5', 'ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', \n",
    "                       'RH_GMvM1', 'RH_GMvS1', 'GMt_M1', 'GMt_S1', 'LH_GM_M1S1', \n",
    "                       'RH_GM_M1S1', 'TGMv_all','ctxGMv_all', 'RH_GMv_all','LH_GMv_all']\n",
    "    df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_used)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    SHAP_features_importance(df_used, kmeans, scaler)\n",
    "\n",
    "run_shap_analysis(df_all_combined, Aging_INDICES, variable='FA', segment='CC5', n_clusters=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "def SHAP_cluster_analysis(df_used, kmeans, scaler, n_clusters):\n",
    "    def model_output(data):\n",
    "        return kmeans.transform(data)\n",
    "    \n",
    "    df_scaled = scaler.transform(df_used)\n",
    "    explainer = shap.KernelExplainer(model_output, df_scaled)\n",
    "    shap_values = explainer.shap_values(df_scaled)\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        print(f\"\\nAnalyse du cluster {cluster} :\")\n",
    "        cluster_indices = np.where(kmeans.labels_ == cluster)[0]\n",
    "        cluster_shap_values = np.mean(np.abs(shap_values)[cluster_indices], axis=0)\n",
    "        shap.summary_plot(shap_values[cluster_indices], features=df_scaled[cluster_indices], feature_names=df_used.columns)\n",
    "        feature_importance_cluster = np.mean(np.abs(shap_values[cluster_indices]), axis=0).mean(axis=1)\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': df_used.columns,\n",
    "            'importance': feature_importance_cluster\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        \n",
    "        print(f\"Top variables qui caractérisent le cluster {cluster} :\\n\")\n",
    "        print(feature_importance_df.head(10))\n",
    "\n",
    "def run_cluster_analysis(df_combined, Aging_INDICES, variable, segment, n_clusters=10):\n",
    "    df_filtered = df_combined[(df_combined['Variable'] == variable) & (df_combined['CC_Segment'] == segment)]\n",
    "    df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1).reset_index(drop=True)\n",
    "    df_used = pd.concat([Aging_INDICES, df_filtered_clean], axis=1, ignore_index=False)\n",
    "    df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "    columns_to_drop = ['sub', 'sex', 'group', 'age', 'FAsigCC4', 'MDsigCC4', 'RDsigCC4', \n",
    "                       'FAsigCC5', 'ADsigCC5', 'RDsigCC5', 'LH_GMvM1', 'LH_GMvS1', \n",
    "                       'RH_GMvM1', 'RH_GMvS1', 'GMt_M1','GMt_S1', 'LH_GM_M1S1', 'RH_GM_M1S1']\n",
    "    df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_used)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    SHAP_cluster_analysis(df_used, kmeans, scaler, n_clusters)\n",
    "\n",
    "# Exécuter l'analyse pour FA et CC5\n",
    "run_cluster_analysis(df_all_combined, Aging_INDICES, variable='FA', segment='CC5', n_clusters=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "..............................................................................................\n",
    "\n",
    "### Youssef\n",
    "\n",
    "**I'm still stuck here. I want to look at the variables that separate the clusters without giving attention to age and understand which variable is responsible for this separation. Additionally, I want to understand which variable allows the grouping within each cluster. Ideally, if one variable helps identify a cluster and another helps create the cluster, I could better understand why the data is distributed in this way. Ask me Daniela for more details :D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import ppscore as pps  \n",
    "\n",
    "important_cols = ['sub', 'sex', 'group', 'JND_P', 'IHD', 'FA_body', 'MD_body','AD_body', 'RD_body']\n",
    "#important_cols = ['sub', 'sex', 'group', 'JND_P', 'IHD', 'FA_body', 'MD_body','AD_body', 'RD_body', 'TGMv_all', 'ctxGMv_all', 'LH_GMv_all','RH_GMv_all']\n",
    "\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "def heatmap(df, target=None):\n",
    "\n",
    "    if target is None:\n",
    "        df = df[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "        title = 'PPS matrix'\n",
    "    else:\n",
    "        df = df[df['y'] == target][['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "        title = f\"PPS matrix for target: {target}\"\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    sns.heatmap(df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=False, square=True, cbar_kws={\"shrink\": .5})\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Target\")\n",
    "    plt.show()\n",
    "\n",
    "def prepare_data_for_clustering(df_combined, Aging_INDICES, variable, segment):\n",
    "    df_filtered = df_combined[(df_combined['Variable'] == variable) & (df_combined['CC_Segment'] == segment)]\n",
    "    df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1).reset_index(drop=True)\n",
    "    df_used = pd.concat([Aging_INDICES[important_cols], df_filtered_clean], axis=1)\n",
    "    df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "    columns_to_drop = ['sub', 'sex', 'group']\n",
    "    df_used_clustering = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_used_clustering)\n",
    "    return df_used, df_scaled\n",
    "\n",
    "def apply_kmeans(df_used, df_scaled, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_used['Cluster'] = kmeans.fit_predict(df_scaled)\n",
    "    df_used['cluster'] = df_used['Cluster'].apply(lambda x: f'Cluster_{x}')\n",
    "    df_used.drop('Cluster', axis=1, inplace=True)\n",
    "    return df_used\n",
    "\n",
    "def analyze_pps(df_used):\n",
    "    columns_to_drop = ['sub', 'sex', 'group']\n",
    "    df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "    pps_matrix = pps.matrix(df_used)\n",
    "    heatmap(pps_matrix)\n",
    "    \n",
    "    pps_scores = []\n",
    "    for feature in df_used.columns:\n",
    "        if feature != 'cluster': \n",
    "            pps_score = pps.score(df_used, feature, \"cluster\")\n",
    "            pps_scores.append((feature, pps_score[\"ppscore\"]))\n",
    "\n",
    "    pps_scores_df = pd.DataFrame(pps_scores, columns=[\"Feature\", \"PPS Score\"]).sort_values(by=\"PPS Score\", ascending=False)\n",
    "    pps_scores_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 des caractéristiques les plus contributives à la séparation des clusters :\\n\")\n",
    "    print(pps_scores_df.head(10))\n",
    "\n",
    "    return pps_scores_df\n",
    "\n",
    "def run_cluster_analysis(df_combined, Aging_INDICES, variable, segment):\n",
    "    df_used, df_scaled = prepare_data_for_clustering(df_combined, Aging_INDICES, variable, segment)\n",
    "    df_used = apply_kmeans(df_used, df_scaled, n_clusters=10)\n",
    "    analyze_pps(df_used)\n",
    "\n",
    "run_cluster_analysis(df_all_combined, Aging_INDICES, variable='FA', segment='CC5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import ppscore as pps  # Assurez-vous d'installer ppscore : pip install ppscore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Liste des colonnes importantes pour l'analyse\n",
    "important_cols = ['sub', 'sex', 'group', 'JND_P', 'IHD', 'FA_body', 'MD_body',\n",
    "                  'AD_body', 'RD_body', 'TGMv_all', 'ctxGMv_all', 'LH_GMv_all', 'RH_GMv_all']\n",
    "\n",
    "# Fonction d'imputation de la médiane pour les valeurs manquantes\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "# Fonction pour générer et afficher la heatmap PPS\n",
    "def heatmap(df, target=None):\n",
    "    if target is None:\n",
    "        df = df[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "        title = 'PPS matrix'\n",
    "    else:\n",
    "        df = df[df['y'] == target][['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "        title = f\"PPS matrix for target: {target}\"\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    sns.heatmap(df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=False, square=True, cbar_kws={\"shrink\": .5})\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Target\")\n",
    "    plt.show()\n",
    "\n",
    "# Préparation des données pour le clustering\n",
    "def prepare_data_for_clustering(df_combined, Aging_INDICES, variable, segment):\n",
    "    df_filtered = df_combined[(df_combined['Variable'] == variable) & (df_combined['CC_Segment'] == segment)]\n",
    "    df_filtered_clean = df_filtered.drop(['Variable', 'CC_Segment', 'Group'], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Merge avec les colonnes importantes\n",
    "    df_used = pd.concat([Aging_INDICES[important_cols], df_filtered_clean], axis=1)\n",
    "    \n",
    "    # Imputation de la médiane pour les valeurs manquantes\n",
    "    df_used = df_used.apply(lambda col: impute_median(col) if col.dtype in ['float64', 'int64'] else col)\n",
    "    \n",
    "    # Suppression des colonnes non pertinentes pour le clustering\n",
    "    columns_to_drop = ['sub', 'sex', 'group']\n",
    "    df_used_clustering = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Standardisation des données\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_used_clustering)\n",
    "    \n",
    "    return df_used, df_scaled\n",
    "\n",
    "# Appliquer KMeans pour créer des clusters\n",
    "def apply_kmeans(df_used, df_scaled, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_used['Cluster'] = kmeans.fit_predict(df_scaled)\n",
    "    \n",
    "    # Renommer les clusters pour un affichage clair\n",
    "    df_used['cluster'] = df_used['Cluster'].apply(lambda x: f'Cluster_{x}')\n",
    "    df_used.drop('Cluster', axis=1, inplace=True)\n",
    "    \n",
    "    return df_used\n",
    "\n",
    "# Analyse des caractéristiques avec PPS et affichage de la heatmap\n",
    "def analyze_pps(df_used):\n",
    "    # Suppression des colonnes non pertinentes pour l'analyse PPS\n",
    "    columns_to_drop = ['sub', 'sex', 'group']\n",
    "    df_used = df_used.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Calcul de la matrice PPS\n",
    "    pps_matrix = pps.matrix(df_used)\n",
    "    heatmap(pps_matrix)\n",
    "    \n",
    "    # Calcul du score PPS pour chaque feature par rapport à 'cluster'\n",
    "    pps_scores = []\n",
    "    for feature in df_used.columns:\n",
    "        if feature != 'cluster':  # Ignorer la colonne cluster\n",
    "            pps_score = pps.score(df_used, feature, \"cluster\")\n",
    "            pps_scores.append((feature, pps_score[\"ppscore\"]))\n",
    "\n",
    "    # Créer un DataFrame des scores PPS\n",
    "    pps_scores_df = pd.DataFrame(pps_scores, columns=[\"Feature\", \"PPS Score\"]).sort_values(by=\"PPS Score\", ascending=False)\n",
    "    pps_scores_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Afficher les 10 meilleures caractéristiques\n",
    "    print(f\"\\nTop 10 des caractéristiques les plus contributives à la séparation des clusters :\\n\")\n",
    "    print(pps_scores_df.head(10))\n",
    "\n",
    "    return pps_scores_df\n",
    "\n",
    "# Fonction principale pour l'analyse des clusters\n",
    "def run_cluster_analysis(df_combined, Aging_INDICES, variable, segment):\n",
    "    # Préparer les données pour FA et CC5\n",
    "    df_used, df_scaled = prepare_data_for_clustering(df_combined, Aging_INDICES, variable, segment)\n",
    "    \n",
    "    # Appliquer KMeans pour trouver les clusters\n",
    "    df_used = apply_kmeans(df_used, df_scaled, n_clusters=10)\n",
    "    \n",
    "    # Analyser les caractéristiques qui contribuent aux clusters avec PPS\n",
    "    analyze_pps(df_used)\n",
    "\n",
    "# Exécution du script pour la variable 'FA' et le segment 'CC5'\n",
    "run_cluster_analysis(df_all_combined, Aging_INDICES, variable='FA', segment='CC5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
